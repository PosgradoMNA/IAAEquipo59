{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFj0sSM06dYa"
      },
      "source": [
        "#**Maestría en Inteligencia Artificial Aplicada**\n",
        "##**Curso: Inteligencia Artificial y Aprendizaje Automático**\n",
        "###Tecnológico de Monterrey\n",
        "###Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "## **Adtividad de la Semana 8**\n",
        "###**Máquina de Vector Soporte - Support Vector Machine(SVM)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qgrvy0RGB9XI"
      },
      "source": [
        "**Nombres y matrículas de los integrantes del equipo:**\n",
        "\n",
        "* Elber Aguilar Pérez - A01793568\n",
        "* Jose Francisco Muñoz Del Angel - A01794174  \n",
        "* Daniel Roberto Meneses León - A01794274\n",
        "* David Mireles Samaniego - A01302935 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrJ2ahMODVj1"
      },
      "source": [
        "En cada sección deberás incluir todas las líneas de código necesarias para responder a cada uno de los ejercicios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "exXsscs-Dh-2"
      },
      "outputs": [],
      "source": [
        "# Incluye aquí todos módulos, librerías y paquetes que requieras.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import sklearn\n",
        "from sklearn import datasets\n",
        "from sklearn import svm\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix, plot_roc_curve, recall_score, roc_auc_score\n",
        "from sklearn.metrics import RocCurveDisplay  \n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import  cross_validate,  RepeatedStratifiedKFold, learning_curve, validation_curve \n",
        "from sklearn.metrics import fbeta_score, make_scorer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X25brD-gQdZM"
      },
      "source": [
        "#**Ejercicio-1.** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3nU2GuWYCy6C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "f1d9c60c-a2c7-4d82-adba-1287339b136b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   A1     A2     A3  A4  A5  A6     A7  A8  A9  A10  A11  A12  A13   A14  A15\n",
              "0   1  22.08  11.46   2   4   4  1.585   0   0    0    1    2  100  1213    0\n",
              "1   0  22.67   7.00   2   8   4  0.165   0   0    0    0    2  160     1    0\n",
              "2   0  29.58   1.75   1   4   4  1.250   0   0    0    1    2  280     1    0\n",
              "3   0  21.67  11.50   1   5   3  0.000   1   1   11    1    2    0     1    1\n",
              "4   1  20.17   8.17   2   6   4  1.960   1   1   14    0    2   60   159    1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ebc0f5c8-1c01-4288-afbf-095436d345e7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A1</th>\n",
              "      <th>A2</th>\n",
              "      <th>A3</th>\n",
              "      <th>A4</th>\n",
              "      <th>A5</th>\n",
              "      <th>A6</th>\n",
              "      <th>A7</th>\n",
              "      <th>A8</th>\n",
              "      <th>A9</th>\n",
              "      <th>A10</th>\n",
              "      <th>A11</th>\n",
              "      <th>A12</th>\n",
              "      <th>A13</th>\n",
              "      <th>A14</th>\n",
              "      <th>A15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>22.08</td>\n",
              "      <td>11.46</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1.585</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>100</td>\n",
              "      <td>1213</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>22.67</td>\n",
              "      <td>7.00</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>0.165</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>160</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>29.58</td>\n",
              "      <td>1.75</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1.250</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>280</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>21.67</td>\n",
              "      <td>11.50</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>20.17</td>\n",
              "      <td>8.17</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>1.960</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>60</td>\n",
              "      <td>159</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ebc0f5c8-1c01-4288-afbf-095436d345e7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ebc0f5c8-1c01-4288-afbf-095436d345e7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ebc0f5c8-1c01-4288-afbf-095436d345e7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "column_names = [\"A1\", \"A2\",\"A3\",\"A4\", \"A5\", \"A6\", \"A7\", \"A8\", \"A9\", \"A10\", \"A11\", \"A12\", \"A13\", \"A14\", \"A15\"]\n",
        "\n",
        "file = 'https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/australian/australian.dat'\n",
        "df = pd.read_csv(file, sep=\" \", header=None, names=column_names)\n",
        "df = pd.DataFrame(df)\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD1hVA5DkIDX",
        "outputId": "e9780e14-b716-49f1-ac06-f6f618101bcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(586, 14) : dimensión de datos de entrada para entrenamiento\n",
            "(104, 14) : dimensión de datos de entrada para prueba\n",
            "(586,) : dimensión de variable de salida para entrenamiento\n",
            "(104,) : dimensión de variable de salida para prueba\n"
          ]
        }
      ],
      "source": [
        "# Split data into train and test sets\n",
        "\n",
        "X = df.iloc[:,:-1]\n",
        "y = df.iloc[:,-1]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)\n",
        "\n",
        "print(X_train.shape, ': dimensión de datos de entrada para entrenamiento')\n",
        "print(X_test.shape, ': dimensión de datos de entrada para prueba')  \n",
        "\n",
        "print(y_train.shape, ': dimensión de variable de salida para entrenamiento')\n",
        "print(y_test.shape, ': dimensión de variable de salida para prueba')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dabAV_FdkIDX",
        "outputId": "4588f75a-6c7e-4047-c02e-ed1c90100f24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0\n",
              "1      0\n",
              "2      0\n",
              "3      1\n",
              "4      1\n",
              "      ..\n",
              "685    1\n",
              "686    0\n",
              "687    1\n",
              "688    1\n",
              "689    1\n",
              "Name: A15, Length: 690, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZhr2hkECzVv"
      },
      "source": [
        "#**Ejercicio-2.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kGfAoOPkC1PP"
      },
      "outputs": [],
      "source": [
        "# Create the preprocessing pipelines for both numeric and categorical data.\n",
        "\n",
        "numeric_features = ['A2','A3','A7','A10','A13','A14']\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[('impmedian', SimpleImputer(strategy='median')),\n",
        "                                      ('scaler', MinMaxScaler(feature_range=(1,2)))])\n",
        "\n",
        "categorical_features = ['A1', 'A4', 'A5', 'A6', 'A8', 'A9', 'A11', 'A12']\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[('impModa', SimpleImputer(strategy='most_frequent'))])\n",
        "\n",
        "categorical_features_ohe = ['A4','A5','A6','A12']\n",
        "\n",
        "categorical_transformer_ohe = Pipeline(steps=[('onehote', OneHotEncoder(handle_unknown= 'ignore'))])\n",
        "\n",
        "columnTransformer = ColumnTransformer(transformers=[('numeric', numeric_transformer, numeric_features),\n",
        "                                                    ('categorical', categorical_transformer, categorical_features),\n",
        "                                                    ('carohe', categorical_transformer_ohe, categorical_features_ohe)],\n",
        "                                     remainder='passthrough')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCunuooTC2W3"
      },
      "source": [
        "#**Ejercicio-3.**\n",
        "\n",
        "Utiliza la función Dummy para modelos de clasificación con la estrategia “stratified” con el conjunto\n",
        "que tienes de datos de entrenamiento y validación. Obtener los valores de partida (baseline) de las\n",
        "siguientes métricas: accuracy, f1-score, precision y recall. ¿Por qué consideras que es adecuado usar\n",
        "la estrategia “stratified” en este caso?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8q71FL-kIDZ",
        "outputId": "824cc195-22a9-40fb-aaa0-2587ef4f6502"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.46\n",
            "Precision: 0.40\n",
            "Recall: 0.46\n",
            "F1-score: 0.43\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.52      0.47      0.49        58\n",
            "        True       0.40      0.46      0.43        46\n",
            "\n",
            "    accuracy                           0.46       104\n",
            "   macro avg       0.46      0.46      0.46       104\n",
            "weighted avg       0.47      0.46      0.46       104\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = DummyClassifier(strategy='stratified') # Modelo Dummy\n",
        "\n",
        "pipeline = Pipeline(steps=[('preprocessor', columnTransformer),\n",
        "                            ('model', model)])\n",
        "\n",
        "\n",
        "pipe_dummy = pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_prediction = pipe_dummy.predict(X_test)\n",
        "\n",
        "print('Accuracy: {:.2f}'.format(accuracy_score(y_test, y_prediction)))\n",
        "print('Precision: {:.2f}'.format(precision_score(y_test, y_prediction,zero_division=0)))\n",
        "print('Recall: {:.2f}'.format(recall_score(y_test, y_prediction)))\n",
        "print('F1-score: {:.2f}'.format(f1_score(y_test, y_prediction)))\n",
        "print(classification_report(y_test, y_prediction, target_names=['False', 'True'],zero_division=0))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHcFvydfkIDa"
      },
      "source": [
        "## Conclusiones\n",
        "\n",
        "Utilizamos `DummyClassifier` con la estrategia de `Stratified` debido al tipo aleatorio de muestras utilizando el vector de tipo OneHot para distribuciones multinomiales para el tipo especifico de DataFrame que se maneja para este caso, ya que tenemos un gran numero de parametros.\n",
        "\n",
        "Para este ejercicio se generó un modelo de predicción `Dummy`. Se emplea la función `Stratified` para poder realizar un muestreo en donde se divide la población en grupos mas pequeños. Esta función genera una estrategia que genera aleatoriamente y así poder ayudarnos a entender la relación que existen entre estos subgrupos. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chqk9jIDC5Pq"
      },
      "source": [
        "#**Ejercicio-4.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IppUHs8PkIDa"
      },
      "source": [
        "Usando el modelo de máquina de vector soporte (SVM) encuentra sus mejores hiperparámetros\n",
        "con una búsqueda de malla y validación cruzada, desplegando los errores indicados en el ejercicio\n",
        "anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBVSFwK4C6g9"
      },
      "outputs": [],
      "source": [
        "resultados_svc = list() # Almacenamos los resultados de las metricas \n",
        "resultados_grid = list()\n",
        "# Definimos los parámetros a optimizar\n",
        "#svm_params = [{'modelo__kernel':['poly', 'rbf', 'sigmoid', 'linear'],\n",
        "#             'modelo__C':[0.1,1.0], ## np.logspace(-5, 5, 100) # generamos 100 valores para C, se dejan solo 3 por el momento para ahorrar computo\n",
        "#             'modelo__degree':[2,3,4],\n",
        "#             'modelo__class_weight':['balanced', None],\n",
        "#             'modelo__gamma':[1,0.1,0.01] ## np.logspace(-5, 5, 100)\n",
        "#                          }]\n",
        "\n",
        "svm_params = {'modelo__C': [0.1, 1, 10, 100, 1000], \n",
        "              'modelo__gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'modelo__kernel': ['poly', 'rbf', 'sigmoid', 'linear']}\n",
        "\n",
        "metricas =['accuracy', 'f1', 'precision', 'recall']\n",
        "\n",
        "for metrica in metricas:\n",
        "    \n",
        "    modelo_svc = svm.SVC()\n",
        "\n",
        "    pipeline_svc = Pipeline(steps=[('processor', columnTransformer),\n",
        "                               ('modelo', modelo_svc)])\n",
        "\n",
        "\n",
        "    kfold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
        "    search = GridSearchCV(estimator = pipeline_svc, \n",
        "                      param_grid = svm_params, \n",
        "                      cv = kfold,\n",
        "                      scoring= metrica,\n",
        "                      n_jobs = -1)\n",
        "\n",
        "    search.fit(X_train, y_train)\n",
        "    \n",
        "    resultados_svc.append(search.best_estimator_)\n",
        "    resultados_grid.append(search.best_params_)\n",
        "    \n",
        "    print('Los mejores hiperparametros con la metrica de :',metrica, \" son:\")\n",
        "    print('Mejor valor de exactitud obtenido con la mejor combinación:', search.best_score_)\n",
        "    print('Mejor combinación de valores encontrados de los hiperparámetros:', search.best_params_, \"\\n\")\n",
        "    \n",
        "\n",
        "#resultados.append(resultadoDummy)\n",
        "\n",
        "#Se imprimen todas las métricas para datos de Entrenamiento y Prueba\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxuLbCAMkIDa"
      },
      "outputs": [],
      "source": [
        "resultados_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlRi3ka8kIDb"
      },
      "outputs": [],
      "source": [
        "metricas =['accuracy', 'f1', 'precision', 'recall']\n",
        "\n",
        "for i in range(len(resultados_grid)):\n",
        "    modelo = svm.SVC(C=resultados_grid[i]['modelo__C'],\n",
        "                     gamma=resultados_grid[i]['modelo__gamma'],\n",
        "                     kernel=resultados_grid[i]['modelo__kernel']\n",
        "                    )\n",
        "    print(modelo)\n",
        "    pipeline_ = Pipeline(steps=[('processor', columnTransformer),\n",
        "                                   ('modelo', modelo)])\n",
        "\n",
        "\n",
        "    kfold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
        "    for metrica in metricas:\n",
        "\n",
        "        resul_cross_val = cross_validate(pipeline_,\n",
        "                                         X_train, y_train,\n",
        "                                         scoring=metricas,\n",
        "                                         cv=kfold,\n",
        "                                         return_train_score=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2v9wJvokIDb"
      },
      "outputs": [],
      "source": [
        "resul_cross_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv7KFq-mC7PS"
      },
      "source": [
        "#**Ejercicio-5.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaDj3kawC9B6"
      },
      "source": [
        "Obtener además los diagramas de caja y bigote del mejor modelo con todas las métricas calculadas.\n",
        "Incluye tus conclusiones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMiyQ_vWkIDb"
      },
      "outputs": [],
      "source": [
        "sns.set(rc={'figure.figsize':(9,6)})\n",
        "\n",
        "list_test = list()\n",
        "list_test.append(resul_cross_val['test_accuracy'])\n",
        "list_test.append(resul_cross_val['test_f1'])\n",
        "list_test.append(resul_cross_val['test_precision'])\n",
        "list_test.append(resul_cross_val['test_recall'])\n",
        "\n",
        "\n",
        "nombres = ['Accuracy', 'f1', 'Precision','Recall']\n",
        "\n",
        "plt.boxplot(list_test, labels=nombres, showmeans=True)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tQxQROVC9Us"
      },
      "source": [
        "### Conclusiones \n",
        "\n",
        "- Si nuestro enfoque fuese solo reducir los Falsos Negativos sobre el modelo la mejor metrica en este caso sería sin duda alguna Recall.\n",
        "\n",
        "- Tanto el accuracy del modelo como f1-score nos hacen ver que el modelo es bueno pero se pueden mejorar los resultados, si las comparamos con la exactitud obtenida con recall.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2S7LI0NC9wE"
      },
      "source": [
        "#**Ejercicio-6.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6uBleJUC_AU"
      },
      "source": [
        "Verifica que el modelo no esté subentrenado o sobreentrenado mediante alguna de las gráficas de\n",
        "curvas de entrenamiento (aumentando la complejidad o aumentando el conjunto de entrenamiento,\n",
        "la que consideres más adecuada). De ser necesario, realiza los ajustes necesarios para evitar alguno\n",
        "de estos problemas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0P_AcyjC_Dh"
      },
      "outputs": [],
      "source": [
        "# Funciones learning_curve, validation_curve\n",
        "\n",
        "def mi_LearningCurvePlot(train_sizes, train_scores, val_scores):\n",
        "\n",
        "    # Argumentos de entrada de la función mi_LearningCurvePlot:\n",
        "    #      train_sizes : número de observaciones en el conjunto de entrenamiento.\n",
        "    #      train_scores : Exactitud de cada partición en el proceso de Validación-Cruzada (VC) en los datos de entrenamiento.\n",
        "    #                     La dimensión de este conjunto es (pxq) \n",
        "    #                                     donde p=\"número de particiones de manera incremental del conjunto de entrenamiento\".\n",
        "    #                                           q=\"número de particiones de VC\" * \"número de repeticiones de VC con RepitedStratifiedCV\"\n",
        "    #      val_scores : Exactitud de cada partición en el proceso de Validación-Cruzada en los datos de validación.\n",
        "    #                   Es de la misma dimensión que los train_scores. \n",
        "    # Output: la salida es el gráfico con las curvas de aprendizaje.\n",
        "\n",
        "    # Obtenemos los promedios y desviaciones estándar de cada renglón de los resultados de entrenamiento y validación\n",
        "    # La dimensión de cada uno es p=\"número de particiones de manera incremental del conjunto de entrenamiento\".\n",
        "    train_mean = np.mean(train_scores, axis=1)\n",
        "    train_std = np.std(train_scores, axis=1)\n",
        "    val_mean = np.mean(val_scores, axis=1)\n",
        "    val_std = np.std(val_scores, axis=1)\n",
        "\n",
        "    # Graficamos las curvas de aprendizaje incluyendo una región indicando la desviación estándar.\n",
        "    plt.figure(figsize=(7,6))\n",
        "    plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='Training')\n",
        "    plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.1, color='blue')\n",
        "    \n",
        "    plt.plot(train_sizes, val_mean, color='red', marker='+', markersize=5, linestyle='--', label='Validation')\n",
        "    plt.fill_between(train_sizes, val_mean + val_std, val_mean - val_std, alpha=0.1, color='red')\n",
        "    \n",
        "    plt.title('Curvas de Aprendizaje incrementando el tamaño de la muestra')\n",
        "    plt.xlabel('Tamaño del conjunto de entrenamiento')\n",
        "    plt.ylabel('metrica')\n",
        "    plt.grid()\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "def mi_ModeloYCurvas(modelo, Xin, ytv):\n",
        "         \n",
        "    modelo = modelo\n",
        "\n",
        "    pipeline_ = Pipeline(steps=[('processor', columnTransformer),\n",
        "                                   ('modelo', modelo)])\n",
        "\n",
        "\n",
        "    kfold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
        "\n",
        "    delta_train_sz = np.linspace(0.1, 1.0, 50) \n",
        "        \n",
        "    #metricas =['accuracy', 'f1', 'precision', 'recall']\n",
        "        \n",
        "    tr_sizes, tr_scores, val_scores = learning_curve(estimator = pipeline_,    \n",
        "                                                         X = Xin,\n",
        "                                                         y = ytv,\n",
        "                                                         cv = kfold,\n",
        "                                                         train_sizes = delta_train_sz,\n",
        "                                                         random_state=11)\n",
        "\n",
        "           # TERMINA LA SECCIÓN PARA AGREGAR CÓDIGO.\n",
        "           ######################################################################\n",
        "\n",
        "\n",
        "           # Obtenemos el gráfico con las curvas de aprendizaje:\n",
        "    mi_LearningCurvePlot(tr_sizes, tr_scores, val_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8k2WXYkkIDc"
      },
      "outputs": [],
      "source": [
        "metricas =['accuracy', 'f1', 'precision', 'recall']\n",
        "\n",
        "for i in range(len(resultados_grid)):\n",
        "    modelo = svm.SVC(C=resultados_grid[i]['modelo__C'],\n",
        "                     gamma=resultados_grid[i]['modelo__gamma'],\n",
        "                     kernel=resultados_grid[i]['modelo__kernel']\n",
        "                    )\n",
        "    print(modelo)\n",
        "    \n",
        "    mi_ModeloYCurvas(modelo, X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCNGx4TQ8CFI"
      },
      "source": [
        "#**Ejercicio-7.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewvwUcJX78y1"
      },
      "source": [
        "Con los mejores hiperparámetros entontrados encontrar la gráfica de la curva ROC y su área bajo la\n",
        "curva."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWOI6PeckIDc"
      },
      "outputs": [],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bhl7_DNlkIDd"
      },
      "outputs": [],
      "source": [
        "my0s = (y_test==0)   # Se genera el vector Booleano, my0s, con TRUE para las entradas con 0s y FALSE para los 1s. Buscamos quedarnos solo con los negativos.\n",
        "my1s = (y_test==1)\n",
        "\n",
        "print(my0s.shape)\n",
        "print(sum(my0s))   # ambos suman el total del Test set.\n",
        "print(sum(my1s))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNoY95NmkIDd"
      },
      "outputs": [],
      "source": [
        "# definimos la función para crear la curva ROC para el modelo \n",
        "\n",
        "# regresa los TP para un umbral deseado:\n",
        "\n",
        "my0s = (y_test==0)   # Se genera el vector Booleano, my0s, con TRUE para las entradas con 0s y FALSE para los 1s. Buscamos quedarnos solo con los negativos.\n",
        "my1s = (y_test==1)\n",
        "\n",
        "def fun_thTPrate(th, modelo, x, y):\n",
        "    p = modelo.predict_proba(x)   # shape (231x2)\n",
        "    #my1s = (y['A15']==1) \n",
        "    pred1s  = p[my1s][:,1]\n",
        "    return sum(pred1s > th) / sum(my1s)\n",
        "\n",
        "\n",
        "# Análogo ahora con los FP:\n",
        "\n",
        "def fun_thFPrate(th, modelo, x, y):\n",
        "    p = modelo.predict_proba(x)\n",
        "    #my0s = (y['A15']==0) \n",
        "    pred0s  = p[my0s][:,1]\n",
        "    return sum(pred0s>th) / sum(my0s)\n",
        "\n",
        "\n",
        "def fun_roc(TPrate,FPrate, modelo, x, y, rr = None):\n",
        "    \n",
        "    new_th = 0.5 # búsquda del mejor umbral que minimiza el total FP+FN.\n",
        "    rr = 0.36   # búsqueda del mejor radio\n",
        "\n",
        "    TPrate = fun_thTPrate(new_th, modelo, x, y)\n",
        "    FPrate = fun_thFPrate(new_th, modelo, x, y)\n",
        "\n",
        "    RocCurveDisplay.from_estimator(modelo, x, y)\n",
        "    plt.plot([0,0,1,0],[0,1,1,0], 'y--')\n",
        "    plt.hlines(TPrate, 0, FPrate, colors='r', linestyles='dotted')  \n",
        "    plt.vlines(FPrate, 0, TPrate, colors='r', linestyles='dotted')  \n",
        "    \n",
        "    if radio_circulo != None:\n",
        "    # graficando el pedazo de cícrulo:\n",
        "        an = np.linspace(1.5*np.pi, 2*np.pi, 20)\n",
        "        plt.plot(rr*np.cos(an), rr*np.sin(an)+1)\n",
        "        plt.axis('equal')\n",
        "        plt.show() \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1aW0dQX8BLL"
      },
      "outputs": [],
      "source": [
        "for i in range(len(resultados_grid)):\n",
        "    modelo = svm.SVC(C=resultados_grid[i]['modelo__C'],\n",
        "                     gamma=resultados_grid[i]['modelo__gamma'],\n",
        "                     kernel=resultados_grid[i]['modelo__kernel'],\n",
        "                     probability=True\n",
        "                    )\n",
        "    print(modelo)\n",
        "        \n",
        "    pipeline_ = Pipeline(steps=[('processor', columnTransformer),\n",
        "                                   ('modelo', modelo)])\n",
        "    \n",
        "    modelo_SVM = pipeline_.fit(X_train, y_train)\n",
        "    \n",
        "    yhat = modelo_SVM.predict(X_test)\n",
        "\n",
        "    # Obtenemos la tasa de falsos positivos (fpr) y la tasa de verdaderos positivos (tpr):\n",
        "    fpr, tpr, _ = metrics.roc_curve(y_test, yhat)\n",
        "    svm_curve = RocCurveDisplay.from_estimator(modelo_SVM, X_test, y_test)\n",
        "    plt.plot([0,0,1,0],[0,1,1,0], 'y--')\n",
        "    plt.hlines(tpr, 0, fpr, colors='r', linestyles='dotted')  # Se calcula TPrate y FPrate con base a threshold=0.5\n",
        "    plt.vlines(fpr, 0, tpr, colors='r', linestyles='dotted')  # como valor predeteminado\n",
        "    \n",
        "    print('**********')\n",
        "    cmp = ConfusionMatrixDisplay.from_estimator(modelo_SVM, X_test, y_test, cmap='Blues')\n",
        "    cmp.ax_.set_title('Matriz de confusión')\n",
        "    #plt.axis('equal')\n",
        "    #plt.show()\n",
        "    \n",
        "    #fun_roc(tpr, fpr, modelo_SVM, X_test, y_test)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_9l1JbOkIDd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzQn5NR78GFg"
      },
      "source": [
        "#**Ejercicio-8.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBKcp0278IQV"
      },
      "source": [
        "Encontrar el valor del mejor umbral de predicción del modelo que maximice el valor de la métrica\n",
        "f1-score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vp8S0PFs8IMR"
      },
      "outputs": [],
      "source": [
        "for i in range(len(resultados_grid)):\n",
        "    if i == 1:\n",
        "        \n",
        "        modelo = svm.SVC(C=resultados_grid[i]['modelo__C'],\n",
        "                         gamma=resultados_grid[i]['modelo__gamma'],\n",
        "                         kernel=resultados_grid[i]['modelo__kernel'],\n",
        "                         probability=True\n",
        "                        )\n",
        "        print('ROC - F1-Score')\n",
        "\n",
        "        pipeline_ = Pipeline(steps=[('processor', columnTransformer),\n",
        "                                    ('modelo', modelo)])\n",
        "\n",
        "        modelo_SVM = pipeline_.fit(X_train, y_train)\n",
        "\n",
        "        yhat = modelo_SVM.predict(X_test)\n",
        "        \n",
        "        new_th = 0.3 # búsquda del mejor umbral que minimiza el total FP+FN.\n",
        "        rr = 0.36   # búsqueda del mejor radio\n",
        "\n",
        "        new_TPrate = fun_thTPrate(new_th, modelo_SVM, X_test, y_test)\n",
        "        new_FPrate = fun_thFPrate(new_th, modelo_SVM, X_test, y_test)\n",
        "        # Obtenemos la tasa de falsos positivos (fpr) y la tasa de verdaderos positivos (tpr):\n",
        "        #fpr, tpr, _ = metrics.roc_curve(y_test, yhat)\n",
        "        svm_curve = RocCurveDisplay.from_estimator(modelo_SVM, X_test, y_test)\n",
        "        plt.plot([0,0,1,0],[0,1,1,0], 'y--')\n",
        "        plt.hlines(new_TPrate, 0, new_FPrate, colors='r', linestyles='dotted')  # Se calcula TPrate y FPrate con base a threshold=0.5\n",
        "        plt.vlines(new_FPrate, 0, new_TPrate, colors='r', linestyles='dotted')  # como valor predeteminado\n",
        "\n",
        "        print('**********')\n",
        "        cmp = ConfusionMatrixDisplay.from_estimator(modelo_SVM, X_test, y_test, cmap='Blues')\n",
        "        cmp.ax_.set_title('Matriz de confusión')\n",
        "    else:\n",
        "        None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mASNrZWs8JTh"
      },
      "source": [
        "#**Ejercicio-9.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MvtCi6nkIDh"
      },
      "source": [
        "Encontrar el valor del mejor umbral de predicción del modelo que maximice el valor de la métrica gmean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6HJP9hb8LCp"
      },
      "outputs": [],
      "source": [
        "# Función calcular G-Mean\n",
        "\n",
        "def mi_gmean(yreal, ypred):\n",
        "    \n",
        "    vn, fp, fn, vp = confusion_matrix(yreal, ypred).ravel()\n",
        "    \n",
        "    gmean = np.sqrt((mi_recall(yreal, ypred)) * vn / (vn+fp))\n",
        "    \n",
        "    return gmean\n",
        "\n",
        "\n",
        "resultados_svc = list() # Almacenamos los resultados de las metricas \n",
        "resultados_grid_gmean = list()\n",
        "# Definimos los parámetros a optimizar\n",
        "#svm_params = [{'modelo__kernel':['poly', 'rbf', 'sigmoid', 'linear'],\n",
        "#             'modelo__C':[0.1,1.0], ## np.logspace(-5, 5, 100) # generamos 100 valores para C, se dejan solo 3 por el momento para ahorrar computo\n",
        "#             'modelo__degree':[2,3,4],\n",
        "#             'modelo__class_weight':['balanced', None],\n",
        "#             'modelo__gamma':[1,0.1,0.01] ## np.logspace(-5, 5, 100)\n",
        "#                          }]\n",
        "\n",
        "svm_params = {'modelo__C': [0.1, 1, 10, 100, 1000], \n",
        "              'modelo__gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'modelo__kernel': ['poly', 'rbf', 'sigmoid', 'linear']}\n",
        "\n",
        "    \n",
        "modelo_svc = svm.SVC()\n",
        "\n",
        "pipeline_svc = Pipeline(steps=[('processor', columnTransformer),\n",
        "                               ('modelo', modelo_svc)])\n",
        "\n",
        "\n",
        "kfold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
        "search_gmean = GridSearchCV(estimator = pipeline_svc, \n",
        "                     param_grid = svm_params, \n",
        "                     cv = kfold,\n",
        "                     scoring= make_scorer(mi_gmean),\n",
        "                     n_jobs = -1)\n",
        "\n",
        "search_gmean.fit(X_train, y_train)\n",
        "    \n",
        "#resultados_svc.append(search_gmean.best_estimator_)\n",
        "resultados_grid_gmean.append(search_gmean.best_params_)\n",
        "    \n",
        "print('Los mejores hiperparametros para gmean:')\n",
        "print('Mejor valor de exactitud obtenido con la mejor combinación:', search_gmean.best_score_)\n",
        "print('Mejor combinación de valores encontrados de los hiperparámetros:', search_gmean.best_params_, \"\\n\")\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xr6oe1VF8K_A"
      },
      "outputs": [],
      "source": [
        "modelo_svc_gmean = svm.SVC(C=search_gmean.best_params_.get('modelo__C'),\n",
        "                 gamma=search_gmean.best_params_.get('modelo__gamma'),\n",
        "                 kernel=search_gmean.best_params_.get('modelo__kernel'),\n",
        "                 probability=True\n",
        "                )\n",
        "\n",
        "pipeline_ = Pipeline(steps=[('processor', columnTransformer),\n",
        "                                       ('modelo', modelo_svc_gmean)])\n",
        "\n",
        "modelo_SVM = pipeline_.fit(X_train, y_train)\n",
        "\n",
        "yhat = modelo_SVM.predict(X_test)\n",
        "\n",
        "        # Obtenemos la tasa de falsos positivos (fpr) y la tasa de verdaderos positivos (tpr):\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test, yhat)\n",
        "svm_curve = RocCurveDisplay.from_estimator(modelo_SVM, X_test, y_test)\n",
        "plt.plot([0,0,1,0],[0,1,1,0], 'y--')\n",
        "plt.hlines(tpr, 0, fpr, colors='r', linestyles='dotted')  # Se calcula TPrate y FPrate con base a threshold=0.5\n",
        "plt.vlines(fpr, 0, tpr, colors='r', linestyles='dotted')  # como valor predeteminado\n",
        "\n",
        "print('**********')\n",
        "cmp = ConfusionMatrixDisplay.from_estimator(modelo_SVM, X_test, y_test, cmap='Blues')\n",
        "cmp.ax_.set_title('Matriz de confusión')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUIcDshs8MzG"
      },
      "source": [
        "#**Ejercicio-10.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lKNJNIt8N88"
      },
      "source": [
        "## Conclusiones.\n",
        "\n",
        "Con base a los resultados podemos concluir que el mejor modelo clasificador para el tipo de distribucion de nuestros datos es el de `SVM` para distribucion polinomial (`poly`) debido al los resultados de F1 en comparacion de nuestro modelo base (`DummyClassifier`). Asi mismo mediante GridSearch pudimos encontrar los mejores parametros y el tipo de SVM para nuestro caso, esto de igual manera comparado con metricas provistas por nuestra matriz de confusion y aunado a nuestros graficos de curva de aprendizaje, los cuales nos permitieron descartar modelos propuestos con sobre entranmiento (kernel=`linear`). \n",
        "Adicionalmente, gracias a la ayuda del calculo de `gmean` podemos corroborar nuestro grafico ROC, que nos indica el calculo de parametros sobre el umbral para la tasa de verdaderos positivos contra la tasa de verdaderos negativos. Asi mismo, gracias al mismo (grafico ROC) podemos tener una mejor percepcion y posterior calculo de AUC la cual nos indica la taza de cambio del umbra conforme crece el numero de elementos. Que para nuestro modelo seleccionable es aceptable debido a que el AUC es igual `0.91` lo cual nos indica una conversion para la Tasa de Verdaderos Positivos 'estable' para la prediccion de nuevos parametros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ql_r2G-DB_m"
      },
      "source": [
        "###**Fin de la Actividad de la semana 8.**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "2325ea83c19258032bf3fb73843b52d25d307ff4946ede19544cde0714e8af27"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}